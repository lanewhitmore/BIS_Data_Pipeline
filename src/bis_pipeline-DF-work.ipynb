{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5735aab7",
   "metadata": {},
   "source": [
    "# Bank International Settlement - Data Pipeline\n",
    "__Team 6 - Lane Whitmore and Dave Friesen__<br>\n",
    "__ADS-507-02-SP23__<br><br>\n",
    "__GitHub link: https://github.com/lanewhitmore/BIS_Data_Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18aeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = ['Lane Whitmore', 'Dave Friesen']\n",
    "__contact__ = ['lwhitmore@sandiego.edu', 'dfriesen@sandiego.edu']\n",
    "__date__ = '2023-02-04'\n",
    "__license__ = 'MIT'\n",
    "__version__ = '1.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92239d8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bca98b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MySQLdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22972/368264975.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpymysql\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmysql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mMySQLdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Import utility libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'MySQLdb'"
     ]
    }
   ],
   "source": [
    "# Import data and pipeline libraries\n",
    "import pandas as pd\n",
    "import pymysql as mysql\n",
    "import MySQLdb\n",
    "# Import utility libraries\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817837d0",
   "metadata": {},
   "source": [
    "# Data Extract and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d4cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets a simple file row count for data load confirmation\n",
    "def ctrl_count(fname):\n",
    "    f = open(fname)\n",
    "    count = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ca3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full_xru_csv.zip... \n",
      "Processing full_long_cpi_csv.zip... \n",
      "Processing full_cbpol_m_csv.zip... \n"
     ]
    }
   ],
   "source": [
    "# Define file location\n",
    "url = 'https://www.bis.org/statistics/'\n",
    "\n",
    "# Define dataset details\n",
    "ds = {'exr': {'efname': 'full_xru_csv.zip',  # Name of file at above url\n",
    "              'lfname': 'WS_XRU_csv_col.csv',  # Name of unzipped file\n",
    "              'id_vars': range(0, 16)},  # ID (vs. value) columns\n",
    "      'cp': {'efname': 'full_long_cpi_csv.zip',\n",
    "             'lfname': 'WS_LONG_CPI_csv_col.csv',\n",
    "             'id_vars': range(0, 14)},\n",
    "      'pr': {'efname': 'full_cbpol_m_csv.zip',\n",
    "             'lfname': 'WS_CBPOL_M_csv_col.csv',\n",
    "             'id_vars': range(0, 13)}}\n",
    "\n",
    "# Establish dataframe dictionaries - these are accessible by dataset abbreviation and\n",
    "#   this allows for consistent code with flexibility\n",
    "df = {'exr': None, 'cp': None, 'pr': None}  # To be used for full dataframe\n",
    "df_ids = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of IDs only\n",
    "df_values = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of values only\n",
    "\n",
    "# Define download (extract) and load location - In this example, it assumes that the\n",
    "#   default (code) directory is 'src', and that a 'data' directory exists in parallel;\n",
    "#   this can be configured to anything\n",
    "ds_path = '../data/'\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=ds_path+'pipeline.log', filemode='w', force=True,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Iterate dataset\n",
    "for i in ds:\n",
    "    # Show we're doing something\n",
    "    print('Processing '+ds[i]['efname'], end='... ')\n",
    "\n",
    "    # Download file; if error, log and skip to next file in dataset\n",
    "    try:\n",
    "        urlretrieve(url + ds[i]['efname'], ds_path+ds[i]['efname'])\n",
    "        logging.info(ds[i]['efname']+' retrieved')\n",
    "    except urllib.error.HTTPError as e:\n",
    "        logging.error('retrieve_HTTP error_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "    except urllib.error.HTTPException as e:\n",
    "        logging.error('retrieve_HTTP exception_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "    except urllib.error.URLError as e:\n",
    "        logging.error('retrieve_URL error_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "\n",
    "    # Unzip file; if error, log and skip to next file in dataset\n",
    "    try:\n",
    "        with zipfile.ZipFile(ds_path+ds[i]['efname'], 'r') as zip:\n",
    "            zip.extractall(ds_path)\n",
    "        logging.info(ds[i]['efname']+' unzipped')\n",
    "    except FileNotFoundError:\n",
    "        logging.error('unzip_File not found')\n",
    "        continue\n",
    "    except zipfile.BadZipFile:\n",
    "        logging.error('unzip_Bad zip file')\n",
    "        continue\n",
    "    except zipfile.LargeZipFile:\n",
    "        logging.error('unzip_Large zip file')\n",
    "        continue\n",
    "    \n",
    "    # Get control count (row count from raw CSV)\n",
    "    ctrl = ctrl_count(ds_path+ds[i]['lfname'])\n",
    "    \n",
    "    # Load dataframe\n",
    "    df[i] = pd.read_csv(ds_path+ds[i]['lfname'], on_bad_lines='skip', low_memory=False)\n",
    "    \n",
    "    # Confirm control counts\n",
    "    logging.info('{0}_file={1}_import={2}_delta={3}'. \\\n",
    "                 format(ds[i]['lfname'], ctrl, len(df[i]), ctrl-len(df[i])))\n",
    "    \n",
    "    # If control count is off by more than one row (i.e., assuming header),\n",
    "    #   log as warning\n",
    "    if (ctrl - len(df[i])) > 1:\n",
    "        logging.warning('control total exception')\n",
    "        \n",
    "    # The following subsets the full dataframe into two parts: one each for IDs and values\n",
    "    #   This is so that the pivoted values don't repeat all of the IDs unecessarily\n",
    "    #   Both subsets retain the original index so can be joined that way in further\n",
    "    #     processing\n",
    "    df_ids[i] = df[i].iloc[:, ds[i]['id_vars']]\n",
    "    df_values[i] = df[i].iloc[:, max(ds[i]['id_vars'])+1:]\n",
    "    df_values[i] = pd.melt(df_values[i], value_vars = df_values[i].iloc[:, 0:])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3475b9",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c62dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating connection to local machine // formulas to nest df to \n",
    "def run_connection(db_connection: mysql, syntax: str) -> None:\n",
    "    \"\"\"\n",
    "    Run Syntax.\n",
    "\n",
    "    :param db_connection: DB connection object. \n",
    "    :param syntax: Syntax for database connection execution.\n",
    "    \"\"\"\n",
    "    cur = db_connection.cursor()\n",
    "    cur.execute(syntax)\n",
    "    cur.close()\n",
    "\n",
    "def create_table(schema: str, table: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a new table in the connected database on the schema\n",
    "\n",
    "    :param schema: The schema for the table.\n",
    "    :param table: The name of the table within the schema.\n",
    "    \"\"\"\n",
    "    db_conn = mysql.connect(\n",
    "        host=os.environ.get(\"HOST\"), \n",
    "        port=int(os.environ.get(\"PORT\")), \n",
    "        user=os.environ.get(\"USER\"), \n",
    "        passwd=os.environ.get(\"PASSWORD\"), \n",
    "        db=os.environ.get(\"DB_NAME\"),\n",
    "    )\n",
    "    \n",
    "    run_connection(db_connection=db_conn, syntax=f\"CREATE TABLE IF NOT EXISTS {table}({schema})\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    db_conn.close()\n",
    "\n",
    "def populate_table(table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\" \n",
    "    Insert df into table within database\n",
    "\n",
    "    :param table_name: Name of the table within the DB\n",
    "    :param df: df to be inserted into schema/DB\n",
    "    \"\"\"\n",
    "    db_conn = mysql.connect(\n",
    "        host=os.environ.get(\"HOST\"), \n",
    "        port=int(os.environ.get(\"PORT\")), \n",
    "        user=os.environ.get(\"USER\"), \n",
    "        passwd=os.environ.get(\"PASSWORD\"), \n",
    "        db=os.environ.get(\"DB_NAME\"),\n",
    "    )\n",
    "\n",
    "    cur = db_conn.cursor()\n",
    "    cur.execute(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "    cur.close()\n",
    "\n",
    "    col_names = [i[0] for i in cur.description]\n",
    "    df[\"upload_timestamp\"] = [datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")] * len(df.index)\n",
    "\n",
    "    missing_columns = set(col_names).difference(df.columns)\n",
    "    assert not missing_columns, f\"The columns listed are missing from your dataset: {','.join(missing_columns)}\"\n",
    "\n",
    "    df = df[col_names]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        run_connection(db_connection=db_conn, syntax=f\"INSERT INTO {table_name} VALUES{tuple(row.values)}\")\n",
    "    db_conn.commit()\n",
    "    db_conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a80fb81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The columns listed are missing from your dataset: exr_val_id",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22972/1872926410.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpopulate_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"exchange_rate_values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpopulate_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"policy_rate_values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpopulate_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"customer_prices_values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpopulate_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"exchange_rate_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22972/2901834872.py\u001b[0m in \u001b[0;36mpopulate_table\u001b[1;34m(table_name, df)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mmissing_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmissing_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"The columns listed are missing from your dataset: {','.join(missing_columns)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The columns listed are missing from your dataset: exr_val_id"
     ]
    }
   ],
   "source": [
    "populate_table(table_name = \"exchange_rate_values\", df = df_values['exr'])\n",
    "populate_table(table_name = \"policy_rate_values\", df = df_values['pr'])\n",
    "populate_table(table_name = \"customer_prices_values\", df = df_values['cp'])\n",
    "\n",
    "populate_table(table_name = \"exchange_rate_id\", df = df_ids['exr'].iloc[:,[1,3,5,7,9,10,13,14,15,16]])\n",
    "populate_table(table_name = \"policy_rate_id\", df = df_ids['pr'].iloc[:,[1,3,6,7,9,10,11,12,13]])\n",
    "populate_table(table_name = \"customer_prices_id\", df = df_ids['cp'].iloc[:,[1,3,5,13,14]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a892be",
   "metadata": {},
   "source": [
    "# Data Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dac27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cb2049e208c6bf0fac27c261f11ec8b9897a816645b496ab018a24a4816c86d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
