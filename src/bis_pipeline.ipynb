{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5735aab7",
   "metadata": {},
   "source": [
    "# Bank International Settlement - Data Pipeline\n",
    "__Team 6 - Lane Whitmore and Dave Friesen__<br>\n",
    "__ADS-507-02-SP23__<br><br>\n",
    "__GitHub link: https://github.com/lanewhitmore/BIS_Data_Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18aeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = ['Lane Whitmore', 'Dave Friesen']\n",
    "__contact__ = ['lwhitmore@sandiego.edu', 'dfriesen@sandiego.edu']\n",
    "__date__ = '2023-02-04'\n",
    "__license__ = 'MIT'\n",
    "__version__ = '1.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92239d8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bca98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and pipeline libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Import utility libraries\n",
    "import urllib\n",
    "import zipfile\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ee4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set database defaults\n",
    "_host = os.environ.get('HOST')\n",
    "_user = os.environ.get('USER')\n",
    "_port = int(os.environ.get('PORT'))\n",
    "_passwd = os.environ.get('PASSWORD')\n",
    "_db = os.environ.get('DB_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a76d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='pipeline.log', filemode='w', force=True,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817837d0",
   "metadata": {},
   "source": [
    "# Data Extract and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07802c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctrl_count(fname):\n",
    "    \"\"\"\n",
    "    Get simple file row count for data load confirmation\n",
    "    \"\"\"\n",
    "    f = open(fname)\n",
    "    count = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return count\n",
    "\n",
    "def download_and_extract_data(url, filename, path):\n",
    "    \"\"\"\n",
    "    Download file from specified URL and extract to specified path;\n",
    "        True if successful, otherwise False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, os.path.join(path, filename))\n",
    "        with zipfile.ZipFile(os.path.join(path, filename), 'r') as zip:\n",
    "            zip.extractall(path)\n",
    "        return True\n",
    "    except (urllib.error.HTTPError, urllib.error.HTTPException, urllib.error.URLError,\n",
    "            FileNotFoundError, zipfile.BadZipFile, zipfile.LargeZipFile) as e:\n",
    "        logging.error(f'{filename} error: {e}')\n",
    "        return False\n",
    "\n",
    "def load_data(filename, path, id_vars):\n",
    "    \"\"\"\n",
    "    Load CSV into dataframe and return ID and value subsets;\n",
    "        This is so that the pivoted values don't repeat all of the IDs unecessarily\n",
    "        Both subsets retain the original index so can be joined that way in further\n",
    "        processing\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(path, filename), on_bad_lines='skip', low_memory=False)\n",
    "    df_ids = df.iloc[:, id_vars]\n",
    "    df_values = df.iloc[:, max(id_vars)+1:]\n",
    "    df_values = df_values.reset_index()\n",
    "    df_values['index'] = df_values['index'] + 1\n",
    "    df_values = pd.melt(df_values, id_vars='index', value_vars=df_values.iloc[:, 1:])\n",
    "    return df, df_ids, df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54085886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file location\n",
    "url = 'https://www.bis.org/statistics/'\n",
    "\n",
    "# Define dataset details\n",
    "ds = {'exr': {'efname': 'full_xru_csv.zip',  # Name of file at above url\n",
    "              'lfname': 'WS_XRU_csv_col.csv',  # Name of unzipped file\n",
    "              'id_vars': range(0, 17)},  # ID (vs. value) columns\n",
    "      'cp': {'efname': 'full_long_cpi_csv.zip',\n",
    "             'lfname': 'WS_LONG_CPI_csv_col.csv',\n",
    "             'id_vars': range(0, 15)},\n",
    "      'pr': {'efname': 'full_cbpol_m_csv.zip',\n",
    "             'lfname': 'WS_CBPOL_M_csv_col.csv',\n",
    "             'id_vars': range(0, 14)}}\n",
    "\n",
    "# Establish dataframe dictionaries - these are accessible by dataset abbreviation and\n",
    "#   this allows for consistent code with flexibility\n",
    "df = {'exr': None, 'cp': None, 'pr': None}  # To be used for full dataframe\n",
    "df_ids = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of IDs only\n",
    "df_values = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of values only\n",
    "\n",
    "# Define download (extract) and load location - In this example, it assumes that the\n",
    "#   default (code) directory is 'src', and that a 'data' directory exists in parallel;\n",
    "#   this can be configured to anything\n",
    "ds_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ee6575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full_xru_csv.zip... \n",
      "Processing full_long_cpi_csv.zip... \n",
      "Processing full_cbpol_m_csv.zip... \n"
     ]
    }
   ],
   "source": [
    "# Iterate dataset\n",
    "for dataset in ds:\n",
    "    # Show we're doing something\n",
    "    print('Processing '+ds[dataset]['efname'], end='... ')\n",
    "\n",
    "    # Download and extract file; if error, log and skip to next file in dataset\n",
    "    if not download_and_extract_data(url + ds[dataset]['efname'], ds[dataset]['efname'], ds_path):\n",
    "        continue\n",
    "\n",
    "    # Load dataframe\n",
    "    df[dataset], df_ids[dataset], df_values[dataset] = load_data(ds[dataset]['lfname'],\n",
    "                                                                 ds_path, ds[dataset]['id_vars'])\n",
    "\n",
    "    # Confirm control counts\n",
    "    ctrl = ctrl_count(ds_path+ds[dataset]['lfname'])\n",
    "    logging.info('{0}_file={1}_import={2}_delta={3}'. \\\n",
    "                 format(ds[dataset]['lfname'], ctrl, len(df[dataset]), ctrl-len(df[dataset])))\n",
    "\n",
    "    # If control count is off by more than one row (i.e., assuming header),\n",
    "    #   log as warning\n",
    "    if (ctrl - len(df[dataset])) > 1:\n",
    "        logging.warning('control total exception')\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3475b9",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c62dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating connection to local machine // formulas to nest df to \n",
    "def run_connection(db_connection: pymysql, syntax: str) -> None:\n",
    "    \"\"\"\n",
    "    Run Syntax.\n",
    "\n",
    "    :param db_connection: DB connection object. \n",
    "    :param syntax: Syntax for database connection execution.\n",
    "    \"\"\"\n",
    "    cur = db_connection.cursor()\n",
    "    cur.execute(syntax)\n",
    "    cur.close()\n",
    "\n",
    "### Can create a table if need be within notebook\n",
    "def create_table(schema: str, table: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a new table in the connected database on the schema\n",
    "\n",
    "    :param schema: The schema for the table.\n",
    "    :param table: The name of the table within the schema.\n",
    "    \"\"\"\n",
    "    db_conn = pymysql.connect(\n",
    "        host=_host,\n",
    "        user=_user,\n",
    "        port=_port, \n",
    "        passwd=_passwd, \n",
    "        db=_db,\n",
    "    )\n",
    "    run_connection(db_connection=db_conn, syntax=f\"CREATE TABLE IF NOT EXISTS {table}({schema})\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    db_conn.close()\n",
    "\n",
    "### populate the table with new records\n",
    "def populate_table(table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\" \n",
    "    Insert df into table within database\n",
    "\n",
    "    :param table_name: Name of the table within the DB\n",
    "    :param df: df to be inserted into schema/DB\n",
    "    \"\"\"\n",
    "    db_data = 'mysql+mysqldb://'+os.environ.get(\"USER\")+':'+os.environ['PASSWORD']+'@'+os.environ.get(\"HOST\")+':3306/'+os.environ.get(\"DB_NAME\")+'?charset=utf8mb4'\n",
    "    engine = create_engine(db_data)\n",
    "    db_conn = pymysql.connect(\n",
    "        host=_host,\n",
    "        user=_user,\n",
    "        port=_port, \n",
    "        passwd=_passwd, \n",
    "        db=_db,\n",
    "    )\n",
    "\n",
    "    cur = db_conn.cursor()\n",
    "    cur.execute(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "    cur.close()\n",
    "\n",
    "    col_names = [i[0] for i in cur.description]\n",
    "    df[f\"{table_name}_id\"] = np.NaN * len(df.index)\n",
    "\n",
    "    missing_columns = set(col_names).difference(df.columns)\n",
    "    assert not missing_columns, f\"The columns listed are missing from your dataset: {','.join(missing_columns)}\"\n",
    "\n",
    "    df = df[col_names]\n",
    "\n",
    "    df.to_sql(table_name, engine, if_exists = 'append', index = False)\n",
    "    db_conn.commit()\n",
    "    engine.dispose()\n",
    "    db_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954baff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating exchange_rate with exr... \n",
      "Populating consumer_prices with cp... \n",
      "Populating policy_rate with pr... \n",
      "Populating exchange_rate_values with exr... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitm\\AppData\\Local\\Temp/ipykernel_3220/3135383224.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{table_name}_id\"] = np.NaN * len(df.index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Populating consumer_prices_values with cp... \n",
      "Populating policy_rate_values with pr... \n"
     ]
    }
   ],
   "source": [
    "### Table names from Schema for ID\n",
    "table_names = ['exchange_rate','consumer_prices','policy_rate']\n",
    "value_table_names = ['exchange_rate_values','consumer_prices_values','policy_rate_values']\n",
    "\n",
    "for i in zip(df_ids, table_names):\n",
    "    print('Populating ' + i[1] + ' with ' + i[0], end='... ')\n",
    "\n",
    "    populate_table(table_name= i[1], df = df_ids[i[0]])\n",
    "\n",
    "    print()\n",
    "\n",
    "for i in zip(df_values, value_table_names):\n",
    "    print('Populating ' + i[1] + ' with ' + i[0], end='... ')\n",
    "\n",
    "    populate_table(table_name= i[1], df = df_values[i[0]])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a892be",
   "metadata": {},
   "source": [
    "# Data Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56dac27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cb2049e208c6bf0fac27c261f11ec8b9897a816645b496ab018a24a4816c86d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
