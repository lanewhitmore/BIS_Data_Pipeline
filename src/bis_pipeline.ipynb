{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5735aab7",
   "metadata": {},
   "source": [
    "# Bank International Settlement - Data Pipeline\n",
    "__Team 6 - Lane Whitmore and Dave Friesen__<br>\n",
    "__ADS-507-02-SP23__<br><br>\n",
    "__GitHub link: https://github.com/lanewhitmore/BIS_Data_Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18aeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = ['Lane Whitmore', 'Dave Friesen']\n",
    "__contact__ = ['lwhitmore@sandiego.edu', 'dfriesen@sandiego.edu']\n",
    "__date__ = '2023-02-04'\n",
    "__license__ = 'MIT'\n",
    "__version__ = '1.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92239d8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bca98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and pipeline libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import utility libraries\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ee4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lane:\n",
    "_host = os.environ.get('HOST')\n",
    "_user = os.environ.get('USER')\n",
    "_port = int(os.environ.get('PORT'))\n",
    "_passwd = os.environ.get('PASSWORD')\n",
    "_db = os.environ.get('DB_NAME')\n",
    "\n",
    "# Dave:\n",
    "#import pymysql\n",
    "#pymysql.install_as_MySQLdb()\n",
    "#_host = 'localhost'\n",
    "#_user = 'dfriesen'\n",
    "#_port = 3306\n",
    "#_passwd = '507password!'\n",
    "#_db = 'bis_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817837d0",
   "metadata": {},
   "source": [
    "# Data Extract and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d4cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets a simple file row count for data load confirmation\n",
    "def ctrl_count(fname):\n",
    "    f = open(fname)\n",
    "    count = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ca3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full_xru_csv.zip... \n",
      "Processing full_long_cpi_csv.zip... \n",
      "Processing full_cbpol_m_csv.zip... \n"
     ]
    }
   ],
   "source": [
    "# Define file location\n",
    "url = 'https://www.bis.org/statistics/'\n",
    "\n",
    "# Define dataset details\n",
    "ds = {'exr': {'efname': 'full_xru_csv.zip',  # Name of file at above url\n",
    "              'lfname': 'WS_XRU_csv_col.csv',  # Name of unzipped file\n",
    "              'id_vars': range(0, 17)},  # ID (vs. value) columns\n",
    "      'cp': {'efname': 'full_long_cpi_csv.zip',\n",
    "             'lfname': 'WS_LONG_CPI_csv_col.csv',\n",
    "             'id_vars': range(0, 15)},\n",
    "      'pr': {'efname': 'full_cbpol_m_csv.zip',\n",
    "             'lfname': 'WS_CBPOL_M_csv_col.csv',\n",
    "             'id_vars': range(0, 14)}}\n",
    "\n",
    "# Establish dataframe dictionaries - these are accessible by dataset abbreviation and\n",
    "#   this allows for consistent code with flexibility\n",
    "df = {'exr': None, 'cp': None, 'pr': None}  # To be used for full dataframe\n",
    "df_ids = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of IDs only\n",
    "df_values = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of values only\n",
    "\n",
    "# Define download (extract) and load location - In this example, it assumes that the\n",
    "#   default (code) directory is 'src', and that a 'data' directory exists in parallel;\n",
    "#   this can be configured to anything\n",
    "ds_path = '../data/'\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=ds_path+'pipeline.log', filemode='w', force=True,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Iterate dataset\n",
    "for i in ds:\n",
    "    # Show we're doing something\n",
    "    print('Processing '+ds[i]['efname'], end='... ')\n",
    "\n",
    "    # Download file; if error, log and skip to next file in dataset\n",
    "    try:\n",
    "        urlretrieve(url + ds[i]['efname'], ds_path+ds[i]['efname'])\n",
    "        logging.info(ds[i]['efname']+' retrieved')\n",
    "    except urllib.error.HTTPError as e:\n",
    "        logging.error('retrieve_HTTP error_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "    except urllib.error.HTTPException as e:\n",
    "        logging.error('retrieve_HTTP exception_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "    except urllib.error.URLError as e:\n",
    "        logging.error('retrieve_URL error_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "\n",
    "    # Unzip file; if error, log and skip to next file in dataset\n",
    "    try:\n",
    "        with zipfile.ZipFile(ds_path+ds[i]['efname'], 'r') as zip:\n",
    "            zip.extractall(ds_path)\n",
    "        logging.info(ds[i]['efname']+' unzipped')\n",
    "    except FileNotFoundError:\n",
    "        logging.error('unzip_File not found')\n",
    "        continue\n",
    "    except zipfile.BadZipFile:\n",
    "        logging.error('unzip_Bad zip file')\n",
    "        continue\n",
    "    except zipfile.LargeZipFile:\n",
    "        logging.error('unzip_Large zip file')\n",
    "        continue\n",
    "    \n",
    "    # Get control count (row count from raw CSV)\n",
    "    ctrl = ctrl_count(ds_path+ds[i]['lfname'])\n",
    "    \n",
    "    # Load dataframe\n",
    "    df[i] = pd.read_csv(ds_path+ds[i]['lfname'], on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "    \n",
    "    # Confirm control counts\n",
    "    logging.info('{0}_file={1}_import={2}_delta={3}'. \\\n",
    "                 format(ds[i]['lfname'], ctrl, len(df[i]), ctrl-len(df[i])))\n",
    "    \n",
    "    # If control count is off by more than one row (i.e., assuming header),\n",
    "    #   log as warning\n",
    "    if (ctrl - len(df[i])) > 1:\n",
    "        logging.warning('control total exception')\n",
    "        \n",
    "    # The following subsets the full dataframe into two parts: one each for IDs and values\n",
    "    #   This is so that the pivoted values don't repeat all of the IDs unecessarily\n",
    "    #   Both subsets retain the original index so can be joined that way in further\n",
    "    #     processing\n",
    "    df_ids[i] = df[i].iloc[:, ds[i]['id_vars']]\n",
    "    df_values[i] = df[i].iloc[:, max(ds[i]['id_vars'])+1:]\n",
    "    df_values[i] = df_values[i].reset_index()\n",
    "    df_values[i] = pd.melt(df_values[i], id_vars = 'index', value_vars = df_values[i].iloc[:, 1:])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3475b9",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c62dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating connection to local machine // formulas to nest df to \n",
    "def run_connection(db_connection: pymysql, syntax: str) -> None:\n",
    "    \"\"\"\n",
    "    Run Syntax.\n",
    "\n",
    "    :param db_connection: DB connection object. \n",
    "    :param syntax: Syntax for database connection execution.\n",
    "    \"\"\"\n",
    "    cur = db_connection.cursor()\n",
    "    cur.execute(syntax)\n",
    "    cur.close()\n",
    "\n",
    "def create_table(schema: str, table: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a new table in the connected database on the schema\n",
    "\n",
    "    :param schema: The schema for the table.\n",
    "    :param table: The name of the table within the schema.\n",
    "    \"\"\"\n",
    "    db_conn = pymysql.connect(\n",
    "        host=_host,\n",
    "        user=_user,\n",
    "        port=_port, \n",
    "        passwd=_passwd, \n",
    "        db=_db,\n",
    "    )\n",
    "    run_connection(db_connection=db_conn, syntax=f\"CREATE TABLE IF NOT EXISTS {table}({schema})\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    db_conn.close()\n",
    "\n",
    "def populate_table(table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\" \n",
    "    Insert df into table within database\n",
    "\n",
    "    :param table_name: Name of the table within the DB\n",
    "    :param df: df to be inserted into schema/DB\n",
    "    \"\"\"\n",
    "#    db_data = 'mysql+mysqldb://' + _user + ':' + _passwd + '@' + _host + ':' + str(_port) + '/' + _db + '?charset=utf8mb4'\n",
    "    db_data = 'mysql+mysqldb://'+os.environ.get(\"USER\")+':'+os.environ['PASSWORD']+'@'+os.environ.get(\"HOST\")+':3306/'+os.environ.get(\"DB_NAME\")+'?charset=utf8mb4'\n",
    "    engine = create_engine(db_data)\n",
    "    db_conn = pymysql.connect(\n",
    "        host=_host,\n",
    "        user=_user,\n",
    "        port=_port, \n",
    "        passwd=_passwd, \n",
    "        db=_db,\n",
    "    )\n",
    "\n",
    "    cur = db_conn.cursor()\n",
    "    cur.execute(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "    cur.close()\n",
    "\n",
    "    col_names = [i[0] for i in cur.description]\n",
    "    df[f\"{table_name}_id\"] = np.NaN * len(df.index)\n",
    "\n",
    "    missing_columns = set(col_names).difference(df.columns)\n",
    "    assert not missing_columns, f\"The columns listed are missing from your dataset: {','.join(missing_columns)}\"\n",
    "\n",
    "    df = df[col_names]\n",
    "\n",
    "    df.to_sql(table_name, engine, if_exists = 'append', index = False)\n",
    "    db_conn.commit()\n",
    "    engine.dispose()\n",
    "    db_conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a80fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitm\\AppData\\Local\\Temp/ipykernel_17156/146143795.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{table_name}_id\"] = np.NaN * len(df.index)\n"
     ]
    }
   ],
   "source": [
    "populate_table(table_name = \"exchange_rate_values\", df = df_values['exr'])\n",
    "populate_table(table_name = \"policy_rate_values\", df = df_values['pr'])\n",
    "populate_table(table_name = \"consumer_prices_values\", df = df_values['cp'])\n",
    "\n",
    "populate_table(table_name = \"exchange_rate\", df = df_ids['exr'])\n",
    "populate_table(table_name = \"policy_rate\", df = df_ids['pr'])\n",
    "populate_table(table_name = \"consumer_prices\", df = df_ids['cp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a892be",
   "metadata": {},
   "source": [
    "# Data Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56dac27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cb2049e208c6bf0fac27c261f11ec8b9897a816645b496ab018a24a4816c86d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
