{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5735aab7",
   "metadata": {},
   "source": [
    "# Bank International Settlement - Data Pipeline\n",
    "__Team 6 - Lane Whitmore and Dave Friesen__<br>\n",
    "__ADS-507-02-SP23__<br><br>\n",
    "__GitHub link: https://github.com/lanewhitmore/BIS_Data_Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18aeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = ['Lane Whitmore', 'Dave Friesen']\n",
    "__contact__ = ['lwhitmore@sandiego.edu', 'dfriesen@sandiego.edu']\n",
    "__date__ = '2023-02-04'\n",
    "__license__ = 'MIT'\n",
    "__version__ = '1.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92239d8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bca98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and pipeline libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Import utility libraries\n",
    "import urllib\n",
    "import zipfile\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ee4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set database defaults\n",
    "_host = os.environ.get('HOST')\n",
    "_user = os.environ.get('USER')\n",
    "_port = int(os.environ.get('PORT'))\n",
    "_passwd = os.environ.get('PASSWORD')\n",
    "_db = os.environ.get('DB_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a76d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='pipeline.log', filemode='w', force=True,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817837d0",
   "metadata": {},
   "source": [
    "# Data Extract and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07802c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctrl_count(fname):\n",
    "    \"\"\"\n",
    "    Get simple file row count for data load confirmation\n",
    "    \"\"\"\n",
    "    f = open(fname)\n",
    "    count = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return count\n",
    "\n",
    "def download_and_extract_data(url, filename, path):\n",
    "    \"\"\"\n",
    "    Download file from specified URL and extract to specified path;\n",
    "        True if successful, otherwise False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, os.path.join(path, filename))\n",
    "        with zipfile.ZipFile(os.path.join(path, filename), 'r') as zip:\n",
    "            zip.extractall(path)\n",
    "        return True\n",
    "    except (urllib.error.HTTPError, urllib.error.HTTPException, urllib.error.URLError,\n",
    "            FileNotFoundError, zipfile.BadZipFile, zipfile.LargeZipFile) as e:\n",
    "        logging.error(f'{filename} error: {e}')\n",
    "        return False\n",
    "\n",
    "def load_data(filename, path, id_vars):\n",
    "    \"\"\"\n",
    "    Load CSV into dataframe and return ID and value subsets;\n",
    "        This is so that the pivoted values don't repeat all of the IDs unecessarily\n",
    "        Both subsets retain the original index so can be joined that way in further\n",
    "        processing\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(path, filename), on_bad_lines='skip', low_memory=False)\n",
    "    df_ids = df.iloc[:, id_vars]\n",
    "    df_values = df.iloc[:, max(id_vars)+1:]\n",
    "    df_values = df_values.reset_index()\n",
    "    df_values['index'] = df_values['index'] + 1\n",
    "    df_values = pd.melt(df_values, id_vars='index', value_vars=df_values.iloc[:, 1:])\n",
    "    return df, df_ids, df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54085886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file location\n",
    "url = 'https://www.bis.org/statistics/'\n",
    "\n",
    "# Define dataset details\n",
    "ds = {'exr': {'efname': 'full_xru_csv.zip',  # Name of file at above url\n",
    "              'lfname': 'WS_XRU_csv_col.csv',  # Name of unzipped file\n",
    "              'id_vars': range(0, 17)},  # ID (vs. value) columns\n",
    "      'cp': {'efname': 'full_long_cpi_csv.zip',\n",
    "             'lfname': 'WS_LONG_CPI_csv_col.csv',\n",
    "             'id_vars': range(0, 15)},\n",
    "      'pr': {'efname': 'full_cbpol_m_csv.zip',\n",
    "             'lfname': 'WS_CBPOL_M_csv_col.csv',\n",
    "             'id_vars': range(0, 14)}}\n",
    "\n",
    "# Establish dataframe dictionaries - these are accessible by dataset abbreviation and\n",
    "#   this allows for consistent code with flexibility\n",
    "df = {'exr': None, 'cp': None, 'pr': None}  # To be used for full dataframe\n",
    "df_ids = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of IDs only\n",
    "df_values = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of values only\n",
    "\n",
    "# Define download (extract) and load location - In this example, it assumes that the\n",
    "#   default (code) directory is 'src', and that a 'data' directory exists in parallel;\n",
    "#   this can be configured to anything\n",
    "ds_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ee6575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full_xru_csv.zip... \n",
      "Processing full_long_cpi_csv.zip... \n",
      "Processing full_cbpol_m_csv.zip... \n"
     ]
    }
   ],
   "source": [
    "# Iterate dataset\n",
    "for dataset in ds:\n",
    "    # Show we're doing something\n",
    "    print('Processing '+ds[dataset]['efname'], end='... ')\n",
    "\n",
    "    # Download and extract file; if error, log and skip to next file in dataset\n",
    "    if not download_and_extract_data(url + ds[dataset]['efname'], ds[dataset]['efname'], ds_path):\n",
    "        continue\n",
    "\n",
    "    # Load dataframe\n",
    "    df[dataset], df_ids[dataset], df_values[dataset] = load_data(ds[dataset]['lfname'],\n",
    "                                                                 ds_path, ds[dataset]['id_vars'])\n",
    "\n",
    "    # Confirm control counts\n",
    "    ctrl = ctrl_count(ds_path+ds[dataset]['lfname'])\n",
    "    logging.info('{0}_file={1}_import={2}_delta={3}'. \\\n",
    "                 format(ds[dataset]['lfname'], ctrl, len(df[dataset]), ctrl-len(df[dataset])))\n",
    "\n",
    "    # If control count is off by more than one row (i.e., assuming header),\n",
    "    #   log as warning\n",
    "    if (ctrl - len(df[dataset])) > 1:\n",
    "        logging.warning('control total exception')\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3475b9",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da455b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/0yy_5gzj2kn3ln4fjthkpxxh0000gn/T/ipykernel_9521/2738272680.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ids['exr'].rename(columns = {'Frequency': 'frequency', 'Reference area': 'reference_area', 'Currency': 'currency',\n",
      "/var/folders/zk/0yy_5gzj2kn3ln4fjthkpxxh0000gn/T/ipykernel_9521/2738272680.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ids['pr'].rename(columns = {'Frequency': 'frequency', 'DECIMALS': 'decimals', 'SOURCE_REF': 'source_ref', 'SUPP_INFO_BREAKS': 'supp_info_breaks',\n",
      "/var/folders/zk/0yy_5gzj2kn3ln4fjthkpxxh0000gn/T/ipykernel_9521/2738272680.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ids['cp'].rename(columns = {'Frequency': 'frequency', 'Reference area': 'reference_area', 'Unit of measure': 'unit_of_measure',\n"
     ]
    }
   ],
   "source": [
    "### Renaming key columns to be imported into the schema to make them more workable within MySQL\n",
    "df_ids['exr'].rename(columns = {'Frequency': 'frequency', 'Reference area': 'reference_area', 'Currency': 'currency', \n",
    "                                'Collection': 'collection', 'Unit Multiplier': 'unit_multiplier', 'DECIMALS': 'decimals',\n",
    "                                'Availability': 'availability', 'TITLE': 'title', 'Series': 'series'}, inplace = True)\n",
    "\n",
    "df_ids['pr'].rename(columns = {'Frequency': 'frequency', 'DECIMALS': 'decimals', 'SOURCE_REF': 'source_ref', 'SUPP_INFO_BREAKS': 'supp_info_breaks',\n",
    "                               'TITLE': 'title', 'Series': 'series'}, inplace = True)\n",
    "\n",
    "df_ids['cp'].rename(columns = {'Frequency': 'frequency', 'Reference area': 'reference_area', 'Unit of measure': 'unit_of_measure',\n",
    "                               'Series':'series'}, inplace = True)\n",
    "\n",
    "df_values['exr'].rename(columns = {'index': 'exchange_rate_id', 'variable': 'date'}, inplace = True)\n",
    "df_values['pr'].rename(columns = {'index': 'policy_rate_id', 'variable': 'date'}, inplace = True)\n",
    "df_values['cp'].rename(columns = {'index': 'consumer_prices_id', 'variable': 'date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c62dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating connection to local machine // formulas to nest df to \n",
    "def run_connection(db_connection: pymysql, syntax: str) -> None:\n",
    "    \"\"\"\n",
    "    Run Syntax.\n",
    "\n",
    "    :param db_connection: DB connection object. \n",
    "    :param syntax: Syntax for database connection execution.\n",
    "    \"\"\"\n",
    "    cur = db_connection.cursor()\n",
    "    cur.execute(syntax)\n",
    "    cur.close()\n",
    "\n",
    "### Can create a table if need be within notebook\n",
    "def create_table(schema: str, table: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a new table in the connected database on the schema\n",
    "\n",
    "    :param schema: The schema for the table.\n",
    "    :param table: The name of the table within the schema.\n",
    "    \"\"\"\n",
    "    db_conn = pymysql.connect(\n",
    "        host=_host,\n",
    "        user=_user,\n",
    "        port=_port, \n",
    "        passwd=_passwd, \n",
    "        db=_db,\n",
    "    )\n",
    "    run_connection(db_connection=db_conn, syntax=f\"CREATE TABLE IF NOT EXISTS {table}({schema})\")\n",
    "\n",
    "    db_conn.commit()\n",
    "    db_conn.close()\n",
    "\n",
    "### populate the table with new records\n",
    "def populate_table(table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\" \n",
    "    Insert df into table within database\n",
    "\n",
    "    :param table_name: Name of the table within the DB\n",
    "    :param df: df to be inserted into schema/DB\n",
    "    \"\"\"\n",
    "    db_data = 'mysql+mysqldb://'+os.environ.get(\"USER\")+':'+os.environ['PASSWORD']+'@'+os.environ.get(\"HOST\")+':3306/'+os.environ.get(\"DB_NAME\")+'?charset=utf8mb4'\n",
    "    engine = create_engine(db_data)\n",
    "    db_conn = pymysql.connect(\n",
    "        host=_host,\n",
    "        user=_user,\n",
    "        port=_port, \n",
    "        passwd=_passwd, \n",
    "        db=_db,\n",
    "    )\n",
    "\n",
    "    ### connecting to cursor to pull column names in table\n",
    "    cur = db_conn.cursor()\n",
    "    cur.execute(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "    cur.close()\n",
    "\n",
    "    ### obtaining table column names\n",
    "    col_names = [i[0] for i in cur.description]\n",
    "\n",
    "    ### Ensure that updates do not duplicate by syncing table indexes and dataframe indexes\n",
    "    index1 = pd.read_sql(f\"SELECT {table_name}_id FROM {table_name};\", db_conn).index\n",
    "    index2 = df.index\n",
    "    indiff = index2.difference(index1)\n",
    "\n",
    "    ### Extracting indexes not seen in table on database\n",
    "    df = df.loc[indiff]\n",
    "\n",
    "    ### Setting null value to be set auto incrementally\n",
    "    df[f\"{table_name}_id\"] = np.NaN * len(df.index)\n",
    "\n",
    "    ### Identify missing columns\n",
    "    missing_columns = set(col_names).difference(df.columns)\n",
    "    #assert not missing_columns, f\"The columns listed are missing from your dataset: {','.join(missing_columns)}\"\n",
    "    logging.info(df, missing_columns)\n",
    "    \n",
    "    if len(missing_columns) > 0:\n",
    "        logging.error('Missing Columns Critical Error')\n",
    "\n",
    "    ### extracting needed columns from dataframes\n",
    "    df = df[col_names]\n",
    "\n",
    "    df.to_sql(table_name, engine, if_exists = 'append', index = False)\n",
    "    db_conn.commit()\n",
    "    engine.dispose()\n",
    "    db_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "954baff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating exchange_rate with exr... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfriesen/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Frequency', 'Reference area', 'Currency', 'Collection', 'Unit Multiplier', 'DECIMALS', 'Availability', 'TITLE', 'Series'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zk/0yy_5gzj2kn3ln4fjthkpxxh0000gn/T/ipykernel_9521/642065934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Populating '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'... '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpopulate_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zk/0yy_5gzj2kn3ln4fjthkpxxh0000gn/T/ipykernel_9521/3267607710.py\u001b[0m in \u001b[0;36mpopulate_table\u001b[0;34m(table_name, df)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m### extracting needed columns from dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5780\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5782\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5784\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5844\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5845\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5847\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Frequency', 'Reference area', 'Currency', 'Collection', 'Unit Multiplier', 'DECIMALS', 'Availability', 'TITLE', 'Series'] not in index\""
     ]
    }
   ],
   "source": [
    "### Table names from Schema for ID\n",
    "table_names = ['exchange_rate','consumer_prices','policy_rate']\n",
    "value_table_names = ['exchange_rate_values','consumer_prices_values','policy_rate_values']\n",
    "\n",
    "for i in zip(df_ids, table_names):\n",
    "    print('Populating ' + i[1] + ' with ' + i[0], end='... ')\n",
    "\n",
    "    populate_table(table_name= i[1], df = df_ids[i[0]])\n",
    "\n",
    "    print()\n",
    "\n",
    "for i in zip(df_values, value_table_names):\n",
    "    print('Populating ' + i[1] + ' with ' + i[0], end='... ')\n",
    "\n",
    "    populate_table(table_name= i[1], df = df_values[i[0]])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a892be",
   "metadata": {},
   "source": [
    "# Data Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56dac27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cb2049e208c6bf0fac27c261f11ec8b9897a816645b496ab018a24a4816c86d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
