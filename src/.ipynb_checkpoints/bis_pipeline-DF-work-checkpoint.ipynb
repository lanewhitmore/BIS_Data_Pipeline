{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5735aab7",
   "metadata": {},
   "source": [
    "# Bank International Settlement - Data Pipeline\n",
    "__Team 6 - Lane Whitmore and Dave Friesen__<br>\n",
    "__ADS-507-02-SP23__<br><br>\n",
    "__GitHub link: https://github.com/lanewhitmore/BIS_Data_Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18aeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "__authors__ = ['Lane Whitmore', 'Dave Friesen']\n",
    "__contact__ = ['lwhitmore@sandiego.edu', 'dfriesen@sandiego.edu']\n",
    "__date__ = '2023-02-04'\n",
    "__license__ = 'MIT'\n",
    "__version__ = '1.0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92239d8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bca98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and pipeline libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Import utility libraries\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817837d0",
   "metadata": {},
   "source": [
    "# Data Extract and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d4cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets a simple file row count for data load confirmation\n",
    "def ctrl_count(fname):\n",
    "    f = open(fname)\n",
    "    count = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ca3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full_xru_csv.zip... \n",
      "Processing full_long_cpi_csv.zip... \n",
      "Processing full_cbpol_m_csv.zip... \n"
     ]
    }
   ],
   "source": [
    "# Define file location\n",
    "url = 'https://www.bis.org/statistics/'\n",
    "\n",
    "# Define dataset details\n",
    "ds = {'exr': {'efname': 'full_xru_csv.zip',  # Name of file at above url\n",
    "              'lfname': 'WS_XRU_csv_col.csv',  # Name of unzipped file\n",
    "              'id_vars': range(0, 16)},  # ID (vs. value) columns\n",
    "      'cp': {'efname': 'full_long_cpi_csv.zip',\n",
    "             'lfname': 'WS_LONG_CPI_csv_col.csv',\n",
    "             'id_vars': range(0, 14)},\n",
    "      'pr': {'efname': 'full_cbpol_m_csv.zip',\n",
    "             'lfname': 'WS_CBPOL_M_csv_col.csv',\n",
    "             'id_vars': range(0, 13)}}\n",
    "\n",
    "# Establish dataframe dictionaries - these are accessible by dataset abbreviation and\n",
    "#   this allows for consistent code with flexibility\n",
    "df = {'exr': None, 'cp': None, 'pr': None}  # To be used for full dataframe\n",
    "df_ids = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of IDs only\n",
    "df_values = {'exr': None, 'cp': None, 'pr': None}  # To be used for subset of values only\n",
    "\n",
    "# Define download (extract) and load location - In this example, it assumes that the\n",
    "#   default (code) directory is 'src', and that a 'data' directory exists in parallel;\n",
    "#   this can be configured to anything\n",
    "ds_path = '../data/'\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=ds_path+'pipeline.log', filemode='w', force=True,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Iterate dataset\n",
    "for i in ds:\n",
    "    # Show we're doing something\n",
    "    print('Processing '+ds[i]['efname'], end='... ')\n",
    "\n",
    "    # Download file; if error, log and skip to next file in dataset\n",
    "    try:\n",
    "        urlretrieve(url + ds[i]['efname'], ds_path+ds[i]['efname'])\n",
    "        logging.info(ds[i]['efname']+' retrieved')\n",
    "    except urllib.error.HTTPError as e:\n",
    "        logging.error('retrieve_HTTP error_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "    except urllib.error.HTTPException as e:\n",
    "        logging.error('retrieve_HTTP exception_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "    except urllib.error.URLError as e:\n",
    "        logging.error('retrieve_URL error_'+str(e.code)+'_'+e.reason)\n",
    "        continue\n",
    "\n",
    "    # Unzip file; if error, log and skip to next file in dataset\n",
    "    try:\n",
    "        with zipfile.ZipFile(ds_path+ds[i]['efname'], 'r') as zip:\n",
    "            zip.extractall(ds_path)\n",
    "        logging.info(ds[i]['efname']+' unzipped')\n",
    "    except FileNotFoundError:\n",
    "        logging.error('unzip_File not found')\n",
    "        continue\n",
    "    except zipfile.BadZipFile:\n",
    "        logging.error('unzip_Bad zip file')\n",
    "        continue\n",
    "    except zipfile.LargeZipFile:\n",
    "        logging.error('unzip_Large zip file')\n",
    "        continue\n",
    "    \n",
    "    # Get control count (row count from raw CSV)\n",
    "    ctrl = ctrl_count(ds_path+ds[i]['lfname'])\n",
    "    \n",
    "    # Load dataframe\n",
    "    df[i] = pd.read_csv(ds_path+ds[i]['lfname'], on_bad_lines='skip', low_memory=False)\n",
    "    \n",
    "    # Confirm control counts\n",
    "    logging.info('{0}_file={1}_import={2}_delta={3}'. \\\n",
    "                 format(ds[i]['lfname'], ctrl, len(df[i]), ctrl-len(df[i])))\n",
    "    \n",
    "    # If control count is off by more than one row (i.e., assuming header),\n",
    "    #   log as warning\n",
    "    if (ctrl - len(df[i])) > 1:\n",
    "        logging.warning('control total exception')\n",
    "        \n",
    "    # The following subsets the full dataframe into two parts: one each for IDs and values\n",
    "    #   This is so that the pivoted values don't repeat all of the IDs unecessarily\n",
    "    #   Both subsets retain the original index so can be joined that way in further\n",
    "    #     processing\n",
    "    df_ids[i] = df[i].iloc[:, ds[i]['id_vars']]\n",
    "    df_values[i] = df[i].iloc[:, max(ds[i]['id_vars'])+1:]\n",
    "    df_values[i] = pd.melt(df_values[i], value_vars = df_values[i].iloc[:, 0:])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3475b9",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e4ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a892be",
   "metadata": {},
   "source": [
    "# Data Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dac27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff47cd274c8997be5d64cba89aa887a6d1b42c6cd14acdf64f98ed7dc2146a1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
